#include <warbler/lexicon/tokenizer.hpp>

// local headers
#include <warbler/util/print.hpp>

// standard headers
#include <cstring>
#include <cstdlib>
#include <stdexcept>
#include <unordered_map>

#define MAX_KeywordLENGTH	(15)

namespace warbler::lexicon
{
	using source::File;
	using lexicon::Token;

	enum class CharType
	{
		Invalid,
		Null,
		Identifier,
		Separator,
		Dot,
		Digit,
		Operator,
		Quote
	};

	static CharType char_types[256];

	static inline bool _match_str(const char * const text, const char * const compare, const usize size)
	{
		for (usize i = 0; i < size; ++i)
		{
			if (text[i] != compare[i])
				return false;
		}

		return true;
	}

	#define match_str(text, compare) _match_str(text, compare, sizeof(compare))

	TokenType get_identifier_type(const char *text)
	{
		switch (text[0])
		{
			case 'b':
				if (match_str(text + 1, "reak"))
					return TokenType::KeywordBreak;
				break;

			case 'c':
				if (text[1] == 'a')
				{
					if (match_str(text + 2, "se"))
						return TokenType::KeywordCase;
				}
				else if (text[1] == 'o')
				{
					if (match_str(text + 2, "ntinue"))
						return TokenType::KeywordContinue;
				}
				break;

			case 'e':
				switch (text[1])
				{
					case 'l':
						if (match_str(text + 2, "se"))
							return TokenType::KeywordElse;
						break;

					case 'n':
						if (match_str(text + 2, "um"))
							return TokenType::KeywordEnum;
						break;

					case 'x':
						if (match_str(text + 2, "port"))
							return TokenType::KeywordExport;
						break;

					default:
						return TokenType::Identifier;
				}
				break;

			case 'f':
				switch (text[1])
				{
					case 'a':
						if (match_str(text + 2, "lse"))
							return TokenType::KeywordFalse;
						break;
						
					case 'o':
						if (text[2] == 'r' && text[3] == '\0')
							return TokenType::KeywordFor;
						break;

					case 'u':
						if (match_str(text + 2, "nction"))
							return TokenType::KeywordFunction;
						break;

					default:
						return TokenType::Identifier;
				}
				break;

			case 'i':
				if (text[1] == 'f')
				{
					if (text[2] == '\0')
						return TokenType::KeywordIf;
				}
				else if (text[1] == 'm')
				{
					if (match_str(text + 2, "port"))
						return TokenType::KeywordImport;
				}
				break;

			case 'l':
				if (match_str(text + 1, "oop"))
					return TokenType::KeywordLoop;
				break;

			case 'm':
				if (text[1] == 'a')
				{
					if (match_str(text + 2, "tch"))
						return TokenType::KeywordMatch;
				}
				else if (text[1] == 'u')
				{
					if (match_str(text + 2, "t"))
						return TokenType::KeywordMut;
				}
				break;

			case 'p':
				if (text[1] == 'r')
				{
					if (match_str(text + 2, "ivate"))
						return TokenType::KeywordPrivate;
				}
				else if (text[1] == 'u')
				{
					if (match_str(text + 2, "blic"))
						return TokenType::KeywordPublic;
				}
				break;

			case 'r':
				if (match_str(text + 1, "eturn"))
					return TokenType::KeywordReturn;
				break;

			case 's':
				if (match_str(text + 1, "truct"))
					return TokenType::KeywordStruct;
				break;

			case 't':
				switch (text[1])
				{
					case 'h':
						if (match_str(text + 2, "en"))
							return TokenType::KeywordThen;
						break;

					case 'r':
						if (match_str(text + 2, "ue"))
							return TokenType::KeywordTrue;
						break;

					case 'y':
						if (match_str(text + 2, "pe"))
							return TokenType::KeywordType;
						break;
				}
				break;

			case 'u':
				if (match_str(text + 1, "nion"))
					return TokenType::KeywordUnion;
				break;

			case 'v':
				if (match_str(text + 1, "ar"))
					return TokenType::KeywordVar;
				break;

			case 'w':
				if (match_str(text + 1, "hile"))
					return TokenType::KeywordWhile;
				break;
				
			default:
				break;
		}

		return TokenType::Identifier;
	}


	

	static Token get_separator_token(const File& file, const usize start_pos)
	{
		TokenType type;

		switch (file[start_pos])
		{
			case '(':
				type = TokenType::LeftParenthesis;
				break;

			case ')':
				type = TokenType::RightParenthesis;
				break;

			case '[':
				type = TokenType::LeftBracket;
				break;

			case ']':
				type = TokenType::RightBracket;
				break;

			case '{':
				type = TokenType::LeftBrace;
				break;

			case '}':
				type = TokenType::RightBrace;
				break;

			case ',':
				type = TokenType::Comma;
				break;

			case ';':
				type = TokenType::Semicolon;
				break;
			
			default:
				throw std::runtime_error("invalid chracter was passed to get_separator_token: '" + String(1, file[start_pos]) + "'");
		}

		return Token(Token(file, start_pos, 1), type);
	}

	static Result<Token> get_digit_token(const File& file, const usize start_pos)
	{
		auto pos = start_pos;
		usize decimal_count = 0;

		while (true)
		{
			CharType type = get_char_type(file[pos]);

			if (type == CharType::Dot)
			{
				decimal_count += 1;
			}
			else if (type != CharType::Digit)
			{
				break;
			}

			pos += 1;
		}

		auto length = pos - start_pos;

		if (decimal_count > 1)
		{
			print_error(source::Snippet(file, start_pos, length), "only one decimal is allowed in float literal");
			return {};
		}

		auto type = decimal_count > 0
			? TokenType::FloatLiteral
			: TokenType::IntegerLiteral;

		return Token(Token(file, start_pos, length), type);
	}

	

	static Result<Token> get_next_token(const source::File& file, const usize start_pos)
	{
		auto type = get_char_type(file[start_pos]);

		switch (type)
		{
			case CharType::Null:
				break;

			case CharType::Identifier:
				return get_identifier_token(file, start_pos);

			case CharType::Separator:
				return get_separator_token(file, start_pos);

			case CharType::Dot:
				return get_dot_token(file, start_pos);

			case CharType::Digit:
				return get_digit_token(file, start_pos);

			case CharType::Operator:
				return get_operator_token(file, start_pos);

			case CharType::Quote:
				return get_quote_token(file, start_pos);

			case CharType::Invalid:
				print_error(source::Snippet(file, start_pos, 1), "invalid character in source file '" + String(1, file[start_pos]) + "'");
				throw std::exception();
		}

		return Token(Token(file, start_pos, 1), TokenType::EndOfFile);
	}

	Result<Array<Token>> tokenize(const source::File& file)
	{	
		Array<Token> out;

		out.reserve(10);

		usize position = 0;

		do
		{
			position = find_next_position(file, position);

			if (out.size() == out.capacity())
				out.reserve(out.size() * 2);

			auto res = get_next_token(file, position);

			if (!res)
				return {};

			out.emplace_back(res.unwrap());
			position += out.back().token().length();
		}
		while (out.back().type() != TokenType::EndOfFile);

		return out;
	}
}
